# -*- coding: utf-8 -*-
"""pretrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SFBlPrQJn8XfRJdMTpSaEIrdofus7uxt
"""


import random
import chess
import torch

import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import chess.polyglot


p = True

def plot(x,y,x_label,y_label,title):
    fig, ax = plt.subplots()
    ax.plot(x,y, label=y_label)
    ax.legend()
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)
    ax.set_title(title)
    plt.show()
def multiPlot(x,ys,x_label,y_label,title):
    fig, ax = plt.subplots()
    for k in range(len(ys)):
        ax.plot(x,ys[k][0], label= ys[k][1])
    ax.legend()
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)
    ax.set_title(title)
    plt.show()

class ChessValueNet(torch.nn.Module):
    def __init__(self):
        super(ChessValueNet, self).__init__()
        self.C1a = torch.nn.Conv2d(12, 12, 5, padding=2) #8
        self.C2a = torch.nn.Conv2d(12, 16, 5, padding=1) # 6
        self.C3a = torch.nn.Conv2d(16, 20, 5, padding=1) # 4
        self.C4a = torch.nn.Conv2d(16, 100, 4, padding=0) # 1
        self.C1b = torch.nn.Conv2d(12, 6, 15, padding=7) # 8
        self.C2b = torch.nn.Conv2d(6, 20, 8, padding=0) # 1
        self.F1a = torch.nn.Linear(100, 64)
        self.F2a = torch.nn.Linear(64, 30)
        self.F1b = torch.nn.Linear(20, 10)
        self.F = torch.nn.Linear(40, 1)
        self.OUT = torch.nn.Linear(2, 1)
        self.LR = torch.nn.LeakyReLU(negative_slope=0.5)
        self.softmax = torch.nn.Softmax()
    def forward(self, x):
        #xs
        xs = torch.sum(x, dim=(1, 2, 3), keepdim=True)
        xs = xs.view(xs.shape[0], -1)
        #xa
        xa = self.C1a(xa)
        xa = self.LR(xa)
        xa = self.C2a(xa)
        xa = self.LR(xa)
        xa = self.C3a(xa)
        xa = self.LR(xa)
        xa = self.C4a(xa)
        xa = self.LR(xa)
        xa = xa.view(xa.shape[0], -1)  # flatten the feature maps into a long vector
        xa = self.F1(xa)
        xa = self.LR(xa)
        xa = self.F2(xa)
        xa = self.LR(xa)
        # xb
        xb = self.C1b(x)
        xb = self.LR(xb)
        xb = self.C2b(xb)
        xb = self.LR(xb)
        xb = xa.view(xb.shape[0], -1)  # flatten the feature maps into a long vector
        xb = self.F1(xb)
        xb = self.LR(xb)
        #concatenate xa and xb
        x = torch.cat((xa, xb), dim=1)
        x = self.F(x)
        x = self.softmax(x)*50
        print(x)
        print(xs)
        x = torch.cat((x, x2), dim=1)
        x = self.OUT(x)
        return x

def get_cost_function():
    return torch.nn.MSELoss()
def get_optimizer(net, lr, wd, momentum):
    optimizer =  optim.SGD(net.parameters(),lr=lr,weight_decay = wd, momentum=momentum)
    return optimizer

def test(net, data_loader, cost_function, device='cuda:0'):
    samples = 0.
    cumulative_loss = 0.
    p = True
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(data_loader):
            # Load data into GPU
            inputs = inputs.to(device, dtype=torch.float32)

            targets = targets.to(device, dtype=torch.float32)
            # Forward pass
            outputs = net(inputs)
            outputs = outputs.view(outputs.shape[1], -1)

            loss = cost_function(outputs, targets)
            if p:
              print(targets)
              print(outputs)
              print(loss)
              p = False

            samples+=1
            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors
    return cumulative_loss/samples


def train(net,data_loader,optimizer,cost_function, device='cuda:0'):
    samples = 0.
    cumulative_loss = 0.

    for batch_idx, (inputs, targets) in enumerate(data_loader):
    # Load data into GPU
        inputs = inputs.to(device, dtype=torch.float32)
        targets = targets.to(device, dtype=torch.float32)
        outputs = net(inputs)
        outputs = outputs.view(outputs.shape[0], -1)
        loss = cost_function(outputs,targets)

        loss.backward()
        optimizer.step()

        optimizer.zero_grad()
        samples+=1
        cumulative_loss += loss.item()


    return cumulative_loss/samples

def load_data(data_path ,n_data):
    # Load the CSV file and return the data as a list of tuples
    data = []
    with open(data_path, 'r') as file:
        lines = file.readlines()[1:]  # Skip header line
        for line in lines:
            n_data -=1
            fen, evaluation = line.strip().split(',')
            if evaluation[0] == "#":
                sign = 1
                if evaluation[1] == "-":
                    sign = -1
                if float(evaluation[2:]) ==0:
                    evaluation = 0
                else:
                    evaluation = sign*5000/np.maximum(1,1+np.log(float(evaluation[2:])))
            data.append((fen_to_tensor(fen)[0], float(evaluation)/100))
            if n_data <0:
                break
    return data

def get_data(batch_size, data_path, n_data, test_batch_size=100):
    data = load_data(data_path, n_data)
    # Split the dataset into training and validation data
    num_samples = len(data)
    temp_samples = int(num_samples * 0.9 + 1)
    test_samples = num_samples - temp_samples
    temp_data, test_data = torch.utils.data.random_split(data, [temp_samples, test_samples])

    validation_samples =  int(temp_samples * 0.4 + 1)
    training_samples = temp_samples - validation_samples
    training_data, validation_data = torch.utils.data.random_split(temp_data, [training_samples, validation_samples])
    # Initialize dataloaders
    train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)
    val_loader = torch.utils.data.DataLoader(validation_data, batch_size=test_batch_size, shuffle=False)
    test_loader = torch.utils.data.DataLoader(validation_data, batch_size=test_batch_size, shuffle=False)

    return train_loader, val_loader, test_loader

def board_to_tensor(board):
    tensor = np.zeros((1,12, 8, 8), dtype=np.float32)
    values = [1, 3, 3, 5, 9, 20]
    piece_map = board.piece_map()
    for square, piece in piece_map.items():
        piece_type = piece.piece_type
        color = (1 if piece.color else -1)
        piece_value = values[piece_type-1] * color
        layer = int(piece_type-1 + 3*(1+color))
        tensor[0,layer, chess.square_rank(square), chess.square_file(square)] = piece_value
        valid_squares = list(board.attacks(square))
        for s in valid_squares:
            tensor[0,layer, chess.square_rank(s), chess.square_file(s)] += 0.1*color
    return torch.from_numpy(tensor)

def fen_to_tensor(fen):
    board = chess.Board(fen)
    return(board_to_tensor(board))

'''
Input arguments
  batch_size: Size of a mini-batch
  device: GPU where you want to train your network
  weight_decay: Weight decay co-efficient for regularization of weights
  momentum: Momentum for SGD optimizer
  epochs: Number of epochs for training the network
'''

def main(n_data = 10000,
         batch_size=100,
         device='cuda:0',
         learning_rate=0.0003,
         weight_decay=0.,
         momentum=0.2,
         epochs=20,
         K = 0,
         datapath = "data/random_evals.csv",
         ):

    train_loader, val_loader, test_loader = get_data(batch_size, datapath, n_data)
    net = ChessValueNet().to(device, dtype=torch.float32)

    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)

    cost_function = get_cost_function()

    print('Before training:')
    train_loss = test(net, train_loader, cost_function)
    val_loss = test(net, val_loader, cost_function)
    test_loss = test(net, test_loader, cost_function)

    print('\t Training loss {:.5f}'.format(train_loss))
    print('\t Validation loss {:.5f}'.format(val_loss))
    print('\t Test loss {:.5f}'.format(test_loss))
    print('-----------------------------------------------------')
    epoch = [0]
    training_loss = [train_loss]
    validation_loss = [val_loss]
    no_progress = 0
    last_loss = val_loss
    for e in range(epochs):
        train_loss = train(net, train_loader, optimizer, cost_function)
        val_loss = test(net, val_loader, cost_function)
        print('Epoch: {:d}'.format(e+1))
        print('\t Training loss {:.5f}'.format(train_loss))
        print('\t Validation loss {:.5f}'.format(val_loss))
        epoch.append(e+1)
        training_loss.append(train_loss)
        validation_loss.append(val_loss)
        if K != 0 :
            if (val_loss-last_loss)>-0.00002:
                no_progress +=1
            else:
                no_progress = 0
            last_loss = val_loss
            if no_progress >= K:
                print("validation loss did not decrease more than 0.00002 per epoch for "+ str(K) + " consecutive epochs, training stopped at epoch " + str(e+1))
                break

    print('-----------------------------------------------------')

    print('After training:')

    train_loss = test(net, train_loader, cost_function)
    val_loss = test(net, val_loader, cost_function)
    test_loss = test(net, test_loader, cost_function)

    print('\t Training loss {:.5f}'.format(train_loss))
    print('\t Validation loss {:.5f}'.format(val_loss))
    print('\t Test loss {:.5f}'.format(test_loss))
    multiPlot(epoch,[[training_loss,"training_loss"],[validation_loss,"validation_loss"]],"Epochs","loss","loss over epochs")

    print('-----------------------------------------------------')
    return(net)

def save(net):
    password = "pretrain"
    if password !="pretrain":
        print("wrong password")
        return(0)
    torch.save(net.state_dict(), "C:/Users/xiaji/Ã©cole d'ing 2A/Chess AI/pretrained_net")
    print('model saved')

net = main(n_data = 10000, K = 10)

save(net)

data = []
size = 10
with open("sample_data/random_evals.csv", 'r') as file:
    lines = file.readlines()[1:]  # Skip header line
    for line in lines:
        size -=1
        fen, evaluation = line.strip().split(',')
        chess.Board(fen)
        print("value = " + evaluation)
        if size <0:
            break

A = [-5.0000e+01,  3.9300e+00,  9.0400e+00,  2.0200e+00, -2.8990e+01,1.1480e+01,  7.5100e+00, -1.3100e+00,  6.2300e+00,  1.2300e+00,-6.3000e-01, -1.3550e+01,  1.1050e+01, -5.5300e+00, -5.5900e+00,4.4000e-01,  6.3200e+00,  9.8000e-01,  8.7100e+00,  4.9400e+00, 2.0500e+00,  6.0100e+00, -2.3600e+00, -4.5200e+00,  2.7900e+00,2.2800e+00,  1.6000e-01, -1.1200e+00, -2.9531e+01, -5.1000e-01,1.1300e+00]
B = [-1.8223, -1.6571, -1.6433, -1.7023, -2.0642, -1.6610, -1.7141, -1.7122,-1.6669, -1.6866, -1.7004, -1.9089, -1.6905, -1.8538, -1.7023, -1.6787,-1.6335, -1.6709, -1.6315, -1.6846, -1.6689, -1.7240, -1.6905, -1.7004,-1.6728, -1.6610, -1.6925, -1.7082, -1.7161, -1.6532, -1.6709, -1.7200,-1.6925, -1.7436, -1.8439, -1.7358, -1.7200, -1.9207, -1.8321, -1.3306,-1.7082, -1.6905, -1.7200, -1.8774, -1.7102, -1.5902, -1.7102, -1.7436][:31]
N = len(A)
S= 0
for k in range(N):
    S+= (A[k]-B[k])**2
S = S/N**2
print(S)

print(N)
print(len(B))
print(torch.nn.MSELoss()(torch.from_numpy(np.array(A)),torch.from_numpy(np.array(B))))

B = create_chess_tensor(chess.Board())
print(B)