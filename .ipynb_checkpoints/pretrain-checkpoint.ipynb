{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452a1d86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5cb44f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\xiaji\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\xiaji\\anaconda3\\lib\\site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\xiaji\\anaconda3\\lib\\site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\xiaji\\anaconda3\\lib\\site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xiaji\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\xiaji\\anaconda3\\lib\\site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\xiaji\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\xiaji\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3fa60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import chess\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import chess.polyglot\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186e056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x,y,x_label,y_label,title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x,y, label=y_label)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "def multiPlot(x,ys,x_label,y_label,title):\n",
    "    fig, ax = plt.subplots()\n",
    "    for k in range(len(ys)):\n",
    "        ax.plot(x,ys[k][0], label= ys[k][1])\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59ac8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessValueNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessValueNet, self).__init__()\n",
    "        self.C1 = torch.nn.Conv2d(6, 6, 5, padding=0)\n",
    "        self.C2 = torch.nn.Conv2d(6, 16, 3, padding=1)\n",
    "        self.C3 = torch.nn.Conv2d(16, 128, 4, padding=0)\n",
    "        self.F1 = torch.nn.Linear(128, 64)\n",
    "        self.OUT = torch.nn.Linear(64, 1)\n",
    "        self.LR = torch.nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.LR(x)\n",
    "        x = self.C2(x)\n",
    "        x = self.LR(x)\n",
    "        x = self.C3(x)\n",
    "        x = self.LR(x)\n",
    "        x = x.view(x.shape[0], -1)  # flatten the feature maps into a long vector\n",
    "        x = self.F1(x)\n",
    "        x = self.LR(x)\n",
    "        x = self.OUT(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def2a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_function():\n",
    "    return torch.nn.MSELoss()\n",
    "def get_optimizer(net, lr, wd, momentum):\n",
    "    optimizer =  optim.SGD(net.parameters(),lr=lr,weight_decay = wd, momentum=momentum)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6311f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, data_loader, cost_function, model, device='cuda:0'):\n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # Load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Forward pass\n",
    "            outputs = net(inputs, False)\n",
    "      \n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            samples+=inputs.shape[0]\n",
    "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
    "    return cumulative_loss/samples\n",
    "\n",
    "\n",
    "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "  \n",
    "  \n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "    # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = net(inputs, True)\n",
    "    \n",
    "        loss = cost_function(outputs,targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        samples+=inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "    \n",
    "\n",
    "    return cumulative_loss/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0a59dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    # Load the CSV file and return the data as a list of tuples\n",
    "    data = []\n",
    "    size = 2000\n",
    "    with open(data_path, 'r') as file:\n",
    "        lines = file.readlines()[1:]  # Skip header line\n",
    "        for line in lines:\n",
    "            size -=1\n",
    "            fen, evaluation = line.strip().split(',')\n",
    "            if evaluation[0] == \"#\":\n",
    "                sign = 1\n",
    "                if evaluation[1] == \"-\":\n",
    "                    sign = -1\n",
    "                evaluation = sign*10000/(1+np.log(float(evaluation[2:])))\n",
    "            data.append((fen_to_tensor(fen)[0], float(evaluation)/100))\n",
    "            if size <0:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def get_data(batch_size, data_path, test_batch_size=100):\n",
    "    data = load_data(data_path)\n",
    "    # Split the dataset into training and validation data\n",
    "    num_samples = len(data)\n",
    "    temp_samples = int(num_samples * 0.9 + 1)\n",
    "    test_samples = num_samples - temp_samples\n",
    "    temp_data, test_data = torch.utils.data.random_split(data, [temp_samples, test_samples])\n",
    "    \n",
    "    validation_samples =  int(temp_samples * 0.4 + 1)\n",
    "    training_samples = temp_samples - validation_samples\n",
    "    training_data, validation_data = torch.utils.data.random_split(temp_data, [training_samples, validation_samples])\n",
    "    # Initialize dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data, batch_size=test_batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(validation_data, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "#deep eval\n",
    "def board_to_tensor(board):\n",
    "    tensor = np.zeros((1,15, 8, 8), dtype=np.float32)\n",
    "    values = [1, 2.6, 2.7, 4.1, 7.8, 15.4]\n",
    "    piece_map = board.piece_map()\n",
    "    prediction = game_stage(total_value(board))\n",
    "    for square, piece in piece_map.items():\n",
    "        piece_type = piece.piece_type\n",
    "        color = (1 if piece.color else -1)\n",
    "        piece_value = values[piece_type-1] * color\n",
    "        layer = int(piece_type-1 + 3*(1+color))\n",
    "        if piece_type == chess.PAWN:\n",
    "            n = chess.square_rank(square)\n",
    "            if piece.color == False:\n",
    "                n = 7-n\n",
    "            piece_value = pawn_value(n, prediction, (board, square, piece.color))\n",
    "        if piece_type == chess.KING:\n",
    "            n = chess.square_rank(square)\n",
    "            if piece.color == False:\n",
    "                n = 7-n\n",
    "            piece_value += (chess.square_file(square), n, prediction)\n",
    "        tensor[0,layer, chess.square_rank(square), chess.square_file(square)] = piece_value\n",
    "        valid_squares = list(board.attacks(square))\n",
    "        for s in valid_squares:\n",
    "            tensor[0,layer, chess.square_rank(s), chess.square_file(s)] += 0.05*color\n",
    "    turn = (1 if board.turn else -1)\n",
    "    tensor[0, 12, 0, 0] = (0.25 if board.has_kingside_castling_rights(True) else 0)\n",
    "    tensor[0, 12, -1, 0] = (-0.25 if board.has_kingside_castling_rights(False) else 0)\n",
    "    tensor[0, 12, 0, -1] = (0.25 if board.has_queenside_castling_rights(True) else 0)\n",
    "    tensor[0, 12, -1, -1] = (-0.25 if board.has_queenside_castling_rights(False) else 0)\n",
    "    tensor[0, 13, :, :] = turn/640\n",
    "    if board.is_checkmate():\n",
    "        tensor[0, 14, :, :] = -turn\n",
    "    return torch.from_numpy(tensor)\n",
    "\n",
    "def fen_to_tensor(fen):\n",
    "    board = chess.Board(fen)\n",
    "    return(board_to_tensor(board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3f99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input arguments\n",
    "  batch_size: Size of a mini-batch\n",
    "  device: GPU where you want to train your network\n",
    "  weight_decay: Weight decay co-efficient for regularization of weights\n",
    "  momentum: Momentum for SGD optimizer\n",
    "  epochs: Number of epochs for training the network\n",
    "'''\n",
    "\n",
    "def main(batch_size=100, \n",
    "         device='cuda:0', \n",
    "         learning_rate=0.01, \n",
    "         weight_decay=0.000001, \n",
    "         momentum=0.9, \n",
    "         epochs=30,\n",
    "         K = 0,\n",
    "         datapath = \"data/random_evals.csv\",\n",
    "         ):\n",
    "    \n",
    "    train_loader, val_loader, test_loader = get_data(batch_size, datapath)\n",
    "    net = ChessValueNet().to(device)\n",
    "   \n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "  \n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    print('Before training:')\n",
    "    train_loss = test(net, train_loader, cost_function, model)\n",
    "    val_loss = test(net, val_loader, cost_function, model)\n",
    "    test_loss = test(net, test_loader, cost_function, model)\n",
    "\n",
    "    print('\\t Training loss {:.5f}'.format(train_loss))\n",
    "    print('\\t Validation loss {:.5f}'.format(val_loss))\n",
    "    print('\\t Test loss {:.5f}'.format(test_loss))\n",
    "    print('-----------------------------------------------------')\n",
    "    epoch = [0]\n",
    "    training_loss = [train_loss]\n",
    "    validation_loss = [val_loss]\n",
    "    no_progress = 0\n",
    "    last_loss = val_loss\n",
    "    for e in range(epochs):\n",
    "        train_loss = train(net, train_loader, optimizer, cost_function, model)\n",
    "        val_loss = test(net, val_loader, cost_function, model)\n",
    "        print('Epoch: {:d}'.format(e+1))\n",
    "        print('\\t Training loss {:.5f}'.format(train_loss))\n",
    "        print('\\t Validation loss {:.5f}'.format(val_loss))\n",
    "        epoch.append(e+1)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(val_loss)\n",
    "        if K != 0 :\n",
    "            if (val_loss-last_loss)>-0.00002:\n",
    "                no_progress +=1\n",
    "            else:\n",
    "                no_progress = 0\n",
    "            last_loss = val_loss\n",
    "            if no_progress >= K:\n",
    "                print(\"validation loss did not decrease more than 0.00002 per epoch for \"+ str(K) + \" consecutive epochs, training stopped at epoch \" + str(e+1))\n",
    "                break\n",
    "\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    print('After training:')\n",
    "\n",
    "    train_loss = test(net, train_loader, cost_function, model)\n",
    "    val_loss = test(net, val_loader, cost_function, model)\n",
    "    test_loss = test(net, test_loader, cost_function, model)\n",
    "\n",
    "    print('\\t Training loss {:.5f}'.format(train_loss))\n",
    "    print('\\t Validation loss {:.5f}'.format(val_loss))\n",
    "    print('\\t Test loss {:.5f}'.format(test_loss))\n",
    "    multiPlot(epoch,[[training_loss,\"training_loss\"],[validation_loss,\"validation_loss\"]],\"Epochs\",\"loss\",\"loss over epochs\")\n",
    " \n",
    "    print('-----------------------------------------------------')\n",
    "    return(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0383e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(net):\n",
    "    password = \"pretrain\"\n",
    "    if password !=\"pretrain\":\n",
    "        print(\"wrong password\")\n",
    "        return(0)\n",
    "    torch.save(net.state_dict(), \"C:/Users/xiaji/école d'ing 2A/Chess AI/pretrained_net\")\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6c0d898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaji\\AppData\\Local\\Temp/ipykernel_81384/762210738.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  evaluation = sign*10000/(1+np.log(float(evaluation[2:])))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_81384/206771362.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_81384/4255171116.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(batch_size, device, learning_rate, weight_decay, momentum, epochs, K, datapath)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChessValueNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1143\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1145\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 820\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m-> 1143\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "net = main(K = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5e8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
